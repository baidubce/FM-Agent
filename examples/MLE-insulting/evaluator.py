#!/usr/bin/env python3
"""
evaluator_mlebench.py â”€ æ”¯æŒè‡ªåŠ¨è¿è¡Œ user_code.py ç”Ÿæˆ submission.csv å¹¶è¯„æµ‹
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
import re
import os
import time
import subprocess
import json
import traceback
import numpy as np
import shutil
import glob
import ast
from pathlib import Path
from datetime import datetime
from typing import List


# å­˜å‚¨å¾—åˆ°å¥–ç‰Œçš„ submission_csv
def move_and_rename_file(source_path, target_path, backup=True):
    """
    ç§»åŠ¨æ–‡ä»¶å¹¶é‡å‘½å
    
    Args:
        source_path: æºæ–‡ä»¶è·¯å¾„
        target_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«æ–°æ–‡ä»¶åï¼‰
    
    Returns:
        bool: æ“ä½œæ˜¯å¦æˆåŠŸ
    """
    """
    å®‰å…¨çš„æ–‡ä»¶ç§»åŠ¨å’Œé‡å‘½åï¼ŒåŒ…å«å¤‡ä»½åŠŸèƒ½
    
    Args:
        source_path: æºæ–‡ä»¶è·¯å¾„
        target_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
        backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
    
    Returns:
        bool: æ“ä½œæ˜¯å¦æˆåŠŸ
    """
    try:
        source = Path(source_path)
        target = Path(target_path)
        
        # éªŒè¯æºæ–‡ä»¶
        if not source.exists():
            print(f"é”™è¯¯: æºæ–‡ä»¶ä¸å­˜åœ¨ {source_path}")
            return False
        
        if not source.is_file():
            print(f"é”™è¯¯: æºè·¯å¾„ä¸æ˜¯æ–‡ä»¶ {source_path}")
            return False
        
        # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå¤„ç†å†²çª
        if target.exists():
            if backup:
                # åˆ›å»ºå¤‡ä»½
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_path = target.parent / f"{target.stem}_backup_{timestamp}{target.suffix}"
                shutil.move(str(target), str(backup_path))
                print(f"ğŸ“¦ å·²åˆ›å»ºå¤‡ä»½: {backup_path}")
            else:
                # ç›´æ¥è¦†ç›–
                target.unlink()
        
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        target.parent.mkdir(parents=True, exist_ok=True)
        
        # æ‰§è¡Œç§»åŠ¨æ“ä½œ
        shutil.move(str(source), str(target))
        
        # éªŒè¯æ“ä½œæˆåŠŸ
        if target.exists() and not source.exists():
            print(f"âœ… æˆåŠŸ: {source_path} -> {target_path}")
            return True
        else:
            print("âŒ æ“ä½œå¯èƒ½æœªå®Œå…¨æˆåŠŸ")
            return False
            
    except Exception as e:
        print(f"âŒ æ“ä½œå¤±è´¥: {e}")
        return False
    

# ä»æ–‡ä»¶åä¸­æå–submissionçš„idxç¼–å·
def extract_submission_idx(filename) -> int:
    """
    ä»æ–‡ä»¶åä¸­æå–submissionçš„idxç¼–å·
    
    Args:
        filename (str): æ–‡ä»¶åï¼Œå¯ä»¥æ˜¯å®Œæ•´è·¯å¾„æˆ–å•çº¯æ–‡ä»¶å
        
    Returns:
        int: æå–åˆ°çš„idxç¼–å·ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›-1
    """
    # è·å–çº¯æ–‡ä»¶åï¼ˆå»æ‰è·¯å¾„ï¼‰
    basename = os.path.basename(filename)
    
    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ŒåŒ¹é… submission_{æ•°å­—}.csv æˆ– submission.csv
    pattern = r'^submission(?:_(\d+))?\.csv$'
    
    # è¿›è¡ŒåŒ¹é…
    match = re.match(pattern, basename)
    
    if match:
        idx_str = match.group(1)
        if idx_str is not None:
            try:
                return int(idx_str)
            except ValueError:
                return -1
        else:
            return -1  # submission.csv çš„æƒ…å†µ
    else:
        return -1  # ä¸åŒ¹é…ä»»ä½•æ¨¡å¼


def robust_loads(json_str):
    """
    å°è¯•ç”¨ json.loads è§£æï¼Œå¦‚æœå¤±è´¥åˆ™ç”¨ ast.literal_eval å…œåº•
    """
    try:
        return json.loads(json_str)
    except Exception:
        try:
            return ast.literal_eval(json_str)
        except Exception:
            raise  # æœ€åè¿˜æ˜¯æŠ›å‡ºåŸå§‹å¼‚å¸¸


def run_python_script(
    py_path: str, competition_name: str, forced_submission_dir: str, gpu_rank: int = 0, timeout=3*60*60
):
    """
    è¿è¡Œç”¨æˆ·æäº¤çš„ python è„šæœ¬ï¼Œç¡®ä¿åœ¨å…¶æ‰€åœ¨ç›®å½•ä¸‹æ‰§è¡Œï¼Œå¹¶ç›‘æ§GPUå³°å€¼æ˜¾å­˜å ç”¨ã€‚
    è¿”å› submission.csv è·¯å¾„ï¼ˆè‹¥ç”ŸæˆæˆåŠŸï¼‰ï¼Œå¦åˆ™æŠ¥é”™
    """
    work_dir = os.path.dirname(py_path)
    py_basename = os.path.basename(py_path)

    t0 = time.time()
    try:
        proc = subprocess.Popen(
            f"python3 {py_path}",
            # cwd=work_dir,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            encoding="utf-8",
            shell=True,
        )
        out, err = proc.communicate(timeout=timeout)
        t1 = time.time()
        print(f"proc err:{err[-2000:]}")

        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆ submission.csv
        submission_csv_path = os.path.join(forced_submission_dir, competition_name)
        os.makedirs(submission_csv_path, exist_ok=True)
        sub_csv = os.path.join(submission_csv_path, "submission.csv")

        if not os.path.exists(sub_csv):
            return {
                "success": False,
                "error_info": {"run_stderr": err[-1000:] + "\nsubmission_csv not found!"},
                "wall_time": t1 - t0,
                "submission_csv": None,
            }
        return {
            "success": True,
            "error_info": {},
            "wall_time": t1 - t0,
            "submission_csv": sub_csv,
        }

    except subprocess.TimeoutExpired:
        proc.kill()
        proc.wait()
        return {
            "success": False,
            "error_info": {"timeout": "python script run timeout"},
            "wall_time": timeout,
            "submission_csv": None,
        }
    except Exception as ex:
        return {
            "success": False,
            "error_info": {
                "exception": str(ex),
                "exception_type": type(ex).__name__,
                "traceback": traceback.format_exc(),
            },
            "wall_time": 0.0,
            "submission_csv": None,
        }


def run_grader(sub_csv, competition, data_dir, timeout=3600*4):
    """
    è°ƒç”¨ mlebench grade-sample å‘½ä»¤ï¼Œæ•è·å¹¶è§£æè¾“å‡ºï¼Œè¿”å›è¯„æµ‹ json dictã€‚
    è‡ªåŠ¨å¤„ç† grader æ— è¾“å‡ºã€è¾“å‡ºé jsonã€json è§£æå¤±è´¥ç­‰æƒ…å†µã€‚
    å…¼å®¹éæ ‡å‡† JSONï¼ˆå¦‚å•å¼•å·ã€None ç­‰ï¼‰ã€‚
    """
    cmd = [
        "mlebench",
        "grade-sample",
        sub_csv,
        competition ,
        "--data-dir", data_dir
    ]
    try:
        t0 = time.time()
        proc = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            encoding="utf-8",
        )
        out, err = proc.communicate(timeout=timeout)
        t1 = time.time()

        # å°è¯•ä»stdoutå’Œstderrä¸­æå–json
        def extract_json(txt):
            m = re.search(r'(\{[\s\S]*?\})', txt)
            return m.group(1) if m else None

        print(f"err:{err[-2000:]}")
        json_str = extract_json(out) or extract_json(err)
        json_from = 'stdout' if extract_json(out) else ('stderr' if extract_json(err) else None)

        if not json_str:
            return {
                "validity": 0.0,
                "score": "null",
                "medal": "none",
                "above_median": False,
                "error_info": {
                    "grader_stdout": out[-1000:],
                    "grader_stderr": err[-1000:],
                    "reason": "No JSON output found in grader stdout or stderr"
                },
                "meta_data": {},
                "eval_wall_time": t1 - t0,
            }

        # å°è¯•è§£æjsonï¼Œä¼˜å…ˆæ ‡å‡†jsonï¼Œå¤±è´¥åç”¨ast.literal_evalå…œåº•
        try:
            res = robust_loads(json_str)
        except Exception as ex:
            return {
                "validity": 0.0,
                "score": 0.0,
                "medal": "none",
                "above_median": False,
                "error_info": {
                    "grader_stdout": out[-1000:],
                    "grader_stderr": err[-1000:],
                    "json_str": json_str,
                    "exception": str(ex),
                    "traceback": traceback.format_exc(),
                    "reason": f"JSON decode error from {json_from}"
                },
                "meta_data": {},
                "eval_wall_time": t1 - t0,
                "competition_report": {}, 
            }

        print(f"res:\n{res}")
        # æ ¡éªŒå¾—åˆ†æ˜¯å¦æ­£å¸¸
        try:
            score = float(res.get("score", 0.0))
        except Exception as ex:
            return {
                "validity": 0.0,
                "score": "null",
                "medal": "none",
                "above_median": False,
                "error_info": {
                    "grader_stdout": out[-1000:],
                    "grader_stderr": err[-1000:],
                    "json_str": json_str,
                    "exception": str(ex),
                    "traceback": traceback.format_exc(),
                    "reason": f"JSON decode error from {json_from}"
                },
                "meta_data": {},
                "eval_wall_time": t1 - t0,
                "competition_report": res, 
            }

        # æˆåŠŸæå–ä¸è§£æ
        return {
            "validity": float(res.get("valid_submission", False)),
            "score": float(res.get("score", 0.0)),
            "medal": (
                "gold" if res.get("gold_medal") else
                "silver" if res.get("silver_medal") else
                "bronze" if res.get("bronze_medal") else
                "none"
            ),
            "above_median": bool(res.get("above_median", False)),
            "error_info": {},
            "meta_data": res,
            "eval_wall_time": t1 - t0,
            "competition_report": res, 
        }

    except subprocess.TimeoutExpired:
        proc.kill()
        proc.wait()
        return {
            "validity": 0.0,
            "score": 0.0,
            "medal": "none",
            "above_median": False,
            "error_info": {"timeout": "grading process timeout"},
            "meta_data": {},
            "eval_wall_time": timeout,
            "competition_report": {}, 
        }
    except Exception as ex:
        return {
            "validity": 0.0,
            "score": 0.0,
            "medal": "none",
            "above_median": False,
            "error_info": {
                "exception": str(ex),
                "traceback": traceback.format_exc(),
            },
            "meta_data": {},
            "eval_wall_time": 0.0,
            "competition_report": {}, 
        }
    

def evaluate(
    path_user_py, 
    competition_name="detecting-insults-in-social-commentary", 
    forced_submission_dir="/tmp/mle_save_dir/", 
    gpu_rank=0, 
    data_dir="./", 
    timeout=2*60*60, 
):
    # æ£€æŸ¥å½“å‰æ–‡ä»¶å¤¹ä¸‹æ˜¯å¦å·²ç»æœ‰submission_csvï¼Œå­˜åœ¨å°±åˆ é™¤ï¼Œé¿å…ä½¿ç”¨ä¹‹å‰çš„ç»“æœ
    submission_csv_path = os.path.join(forced_submission_dir, competition_name)
    os.makedirs(submission_csv_path, exist_ok=True)
    # åŒ¹é…æ‰€æœ‰ä»¥ submission å¼€å¤´ï¼Œä»¥ .csv ç»“å°¾çš„æ–‡ä»¶
    sub_csv_paths: List[str] = glob.glob(os.path.join(submission_csv_path, "submission*.csv"))
    for sub_csv_path in sub_csv_paths:
        if sub_csv_path and os.path.exists(sub_csv_path):
            os.remove(sub_csv_path)

    lower_is_better = False
    step1 = run_python_script(path_user_py, competition_name, forced_submission_dir, gpu_rank, timeout=timeout)
    print(f"step1:{step1}")
    if not step1["success"]:        
        return {
            "validity": 0.0,
            "score": 0,
            "combined_score": 0.0,  # æ–°å¢
            "medal": "none",
            "above_median": False,
            "error_info": {"run_error": step1["error_info"]},
            "meta_data": {},
            "eval_wall_time": step1["wall_time"],
        }
    print("------------------ step1 run successfully ---------------------")

    try:
        sub_csv_path = step1["submission_csv"]
        # è¯„ä¼°é¢„æµ‹æ–‡ä»¶
        grader_res = run_grader(sub_csv_path, competition_name, data_dir, timeout=timeout)
        grader_res["eval_wall_time"] += step1["wall_time"]
        
        def scale_score_with_medal(raw_score, medal:str = "none"):
            # è°ƒæ•´æ–¹å‘ï¼šè¶Šå¥½è¶Šå¤§
            score = -raw_score if lower_is_better else raw_score  
            # ç”¨ sigmoid å‹ç¼©åˆ° (0,1)
            combined_score = float(1 / (1 + np.exp(-score)))
            if medal == "none":
                return combined_score
            elif medal == "bronze":
                return combined_score + 1.0
            elif medal == "silver":
                return combined_score + 2.0
            elif medal == "gold":
                return combined_score + 3.0
            return 
            
        # å…ˆåˆ¤æ–­æœ‰æ•ˆæ€§ï¼Œæ— æ•ˆç›´æ¥å¾—åˆ†ä¸º0
        if grader_res["validity"] <= 0:
            grader_res["combined_score"] = 0.0
        else:
            grader_res["combined_score"] = scale_score_with_medal(
                grader_res["score"], grader_res.get("medal", "none"))

    finally:
        # å®‰å…¨åˆ é™¤æ‰€æœ‰çš„ submission.csv
        try:
            sub_csv_folder = os.path.join("/".join(sub_csv_path.split("/")[:-1]), "submission_save")
            if not os.path.exists(sub_csv_folder):
                os.makedirs(sub_csv_folder)
            
            program_save_folder = os.path.join("/".join(sub_csv_path.split("/")[:-1]), "program_save")
            if not os.path.exists(program_save_folder):
                os.makedirs(program_save_folder)
            
            # å…¶ä½™æ–‡ä»¶ç›´æ¥åˆ é™¤
            for sub_csv_path in sub_csv_paths:
                if sub_csv_path and os.path.exists(sub_csv_path):
                    os.remove(sub_csv_path)

        except Exception as e:
            # åˆ é™¤å¤±è´¥ä¹Ÿä¸å½±å“è¯„æµ‹æµç¨‹ï¼Œåªæ‰“å°è­¦å‘Š
            print(f"Warning: Failed to remove {sub_csv_path}: {str(e)}")
        
    return grader_res


if __name__ == "__main__":
    result = evaluate("init.py",)
